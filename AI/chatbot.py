import os
from dotenv import load_dotenv
import numpy as np
from langchain_upstage import ChatUpstage, UpstageEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_pinecone import PineconeVectorStore
from langchain.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever
from sentence_transformers import SentenceTransformer
from pinecone import Pinecone, ServerlessSpec

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# API í‚¤ ë° ì„¤ì •
pinecone_api_key = os.environ.get("PINECONE_API_KEY")
embedding_upstage = UpstageEmbeddings(model="embedding-query")
index_name = "samsung"

def create_index(api_key, index_name):
    pc = Pinecone(api_key=api_key)

    if index_name not in pc.list_indexes().names():
        pc.create_index(
            name=index_name,
            dimension=4096,
            metric="cosine",
            spec=ServerlessSpec(cloud="aws", region="us-east-1"),
        )
    index = pc.Index(index_name)
    return index

# 1. Pineconeì—ì„œ ë²¡í„° ì €ì¥ì†Œ ë¶ˆëŸ¬ì˜¤ê¸°
def load_vectorstore(index_name="samsung"):
    vectorstore = PineconeVectorStore(
        index_name=index_name,
        embedding=embedding_upstage,
        text_key="chunk"
    )
    return vectorstore

# 3. ì±—ë´‡ ì‘ë‹µ ìƒì„±
def generate_response(query, index):
    query = query
    llm = ChatUpstage()
    prompt1 = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                """
                ë‹¹ì‹ ì€ í•´ë‹¹ ì§ˆë¬¸ì—ì„œ ë‹¤ë¥¸ ë‹µë³€ì„ í•˜ì§€ ì•Šê³ , í•„ìš”í•œ ë…„ë„ë§Œ ì •í™•í•˜ê²Œ 'YYYY' í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.
                ---
                ì˜ˆì‹œë‹µë³€ : 2024
                ---
                ì£¼ì˜: ì ˆëŒ€ ë‹¤ë¥¸ ë‹µë³€ì„ í•˜ì§€ ë§ê³ , ë…„ë„ë§Œ 'YYYY' í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”. 

                í˜„ì¬ ì—°ë„ëŠ” 2024ë…„ ì…ë‹ˆë‹¤.
                """
            ),
            ("human", "{input}"),
        ]
    )
    chain1 = prompt1 | llm | StrOutputParser()
    response1 = chain1.invoke({"input": query})
    model = SentenceTransformer('paraphrase-mpnet-base-v2')
    
    query_vector = model.encode(query).tolist()
    query_vector = np.pad(query_vector, (0, 4096 - len(query_vector)), 'constant').tolist()
    result_docs = index.query(
        vector=query_vector,
        filter={"year": {"$eq": response1[:4]}},
        top_k=3,  # ìƒìœ„ 3ê°œì˜ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        include_metadata=True  # ë©”íƒ€ë°ì´í„°ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.
    )

    # result_docs = ensemble_retriever.invoke(query)
    prompt2 = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                """
                ---
                CONTEXT:
                {context}
                ë‹¹ì‹ ì€ ì‚¼ì„±ì „ì ë‹¤íŠ¸(DART) ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì·¨ì—… ê´€ë ¨ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ì „ë¬¸ ì±—ë´‡ì…ë‹ˆë‹¤. 
                ëª¨ë“  ì§ˆë¬¸ì— ëŒ€í•´ ì‚¼ì„±ì „ì ë‹¤íŠ¸ ìë£Œì—ì„œ ê²€ìƒ‰í•œ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.
                ê²€ìƒ‰ ëœ ìë£ŒëŠ” ëª¨ë‘ ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ë…„ë„ì˜ ìë£Œì´ë¯€ë¡œ í•´ë‹¹ë…„ë„ ìë£Œê°€ ì—†ë‹¤í•˜ì§€ ë§ê³ , ë‹µë³€í•©ë‹ˆë‹¤.

                ì œê³µëœ ìë£Œ ë‚´ì—ì„œ ì í•©í•œ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ì—¬ ì‘ë‹µí•©ë‹ˆë‹¤.
                ë‹µë³€ì˜ ì •í™•ì„±ê³¼ ê´€ë ¨ì„±ì„ ìœ ì§€í•˜ë©°, ìë£Œì—ì„œ ì–¸ê¸‰ë˜ì§€ ì•Šì€ ì‚¬í•­ì€ ì¶”ì¸¡í•˜ì§€ ì•Šê³  ëª…í™•íˆ ì„¤ëª…í•©ë‹ˆë‹¤.

                ìœ ì € ë©”ì‹œì§€ ì˜ˆì‹œ:

                "ë‚´ê°€ í˜„ì¬ ì‚¼ì„±ì „ì DSë¶€ë¬¸ ë©´ì ‘ì„ ì¤€ë¹„ ì¤‘ì¸ë°, ë©´ì ‘ ë•Œ ì•Œì•„ì•¼ í•  í•´ë‹¹ ë¶€ì„œì˜ ì¬ë¬´ì •ë³´ë‚˜ ìµœì‹  ê¸°ìˆ  ë“±ì„ ì•Œë ¤ì¤˜."

                ëª¨ë²” ì‘ë‹µ ì˜ˆì‹œ:

                "ë‹¤íŠ¸ì— ê³µì‹œëœ ì‚¼ì„±ì „ìì˜ ê°€ì¥ ìµœê·¼ ìë£ŒëŠ” 2024ë…„ë„ ìë£Œì…ë‹ˆë‹¤. í•´ë‹¹ ë…„ë„ì˜ ì¬ë¬´ ì •ë³´ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. 
                ë§¤ì¶œì•¡ì€ XXì¡° ì›ì´ë©°, ì˜ì—…ì´ìµì€ XXì¡° ì›ìœ¼ë¡œ ê¸°ë¡ë˜ì—ˆìŠµë‹ˆë‹¤. ë˜í•œ ê¸°ìˆ ë¡œëŠ” ë¡œë´‡ ë° IoT ë¶€ë¬¸ì„ ìµœê·¼ ì‚¬ì—… ì£¼ì œë¡œ ì„ ì •í•˜ê³  ìˆìŠµë‹ˆë‹¤. 
                í•´ë‹¹ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ê¸°ì—…ì˜ ìµœì‹  ë™í–¥ì„ ë¬»ëŠ” ì§ˆë¬¸ì— 'ì‚¼ì„±ì „ìëŠ” ìµœê·¼ ë¡œë´‡ ë° IoT ë¶€ë¬¸ì— ë§ì€ ê´€ì‹¬ì„ ë‘ê³  ìˆìŠµë‹ˆë‹¤. 
                ì´ë¥¼ í†µí•´ ê¸°ìˆ  í˜ì‹ ê³¼ ì‹ ì‚¬ì—… í™•ì¥ì„ ì´ë£¨ê³ ì í•˜ëŠ” ì „ëµì„ ì—¿ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.'ì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ëŒ€ë‹µí•˜ë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤."

                ì£¼ì˜ : í˜„ì¬ ë…„ë„ëŠ” 2024ë…„ì…ë‹ˆë‹¤
                """
            ),
            ("human", "{input}"),
        ]
    )
    llm = ChatUpstage()
    chain2 = prompt2 | llm | StrOutputParser()
    response2 = chain2.invoke({"context": result_docs, "input": query})
    return response2

# 4. ë©”ì¸ í•¨ìˆ˜
def main():
    # Pineconeì—ì„œ ë²¡í„° ì €ì¥ì†Œ ë¶ˆëŸ¬ì˜¤ê¸°
    vectorstore = load_vectorstore(index_name)
    print("âœ… Pinecone ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ ì™„ë£Œ.")
    
    # ì‚¬ìš©ì ì¿¼ë¦¬ ì…ë ¥
    query = input("ğŸ” ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ")
    index = create_index(pinecone_api_key, index_name)
    response = generate_response(query, index)
    print("ğŸ¤– ì±—ë´‡ ì‘ë‹µ:")
    print(response)

if __name__ == "__main__":
    main()

    