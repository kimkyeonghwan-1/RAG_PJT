import os
import pandas as pd
from dotenv import load_dotenv
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_upstage import UpstageEmbeddings
from pinecone import Pinecone, ServerlessSpec
from langchain_pinecone import PineconeVectorStore
from langchain.schema import Document
from uuid import uuid4
import time

# ğŸŒŸ 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
pinecone_api_key = os.environ.get("PINECONE_API_KEY")
embedding_upstage = UpstageEmbeddings(model="embedding-query")
index_name = "samsung"


# ğŸŒŸ 2. Pinecone ì´ˆê¸°í™” ë° ì„¤ì •
def setup_pinecone(api_key):
    pc = Pinecone(api_key=api_key)
    
    # ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚­ì œ
    if index_name in pc.list_indexes().names():
        pc.delete_index(index_name)
    
    # ìƒˆë¡œìš´ ì¸ë±ìŠ¤ ìƒì„±
    pc.create_index(
        name=index_name,
        dimension=4096,
        metric="cosine",
        spec=ServerlessSpec(cloud="aws", region="us-east-1"),
    )
    return pc.Index(index_name)


# ğŸŒŸ 3. ë°ì´í„° ë¡œë“œ ë° Document ê°ì²´ ìƒì„±
def load_documents(file_path):
    df = pd.read_csv(file_path, encoding='utf-8-sig')
    return [
        Document(
            page_content=row['text'],
            metadata={
                "corp_name": row['corp_name'],
                "report_nm": row['report_nm'],
                "rcept_dt": row['rcept_dt'],
                "year": row['report_nm'][7:11]
            }
        )
        for _, row in df.iterrows()
    ]


# ğŸŒŸ 4. í…ìŠ¤íŠ¸ ì²­í¬ ë¶„í• 
def split_documents(docs):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=100,
        length_function=len,
        separators=["\n\n", "\n", " ", ""]
    )
    
    texts, metas = [], []
    for doc in docs:
        chunks = text_splitter.split_text(doc.page_content)
        metadata = doc.metadata
        
        for i, chunk in enumerate(chunks):
            texts.append(f"{metadata['year']}-{metadata['report_nm'][12:14]} : {chunk}")
            metas.append({
                'chunk_id': i,
                'chunk': f"{metadata['year']}-{metadata['report_nm'][12:14]} : {chunk}",
                **metadata,
            })
    return texts, metas


# ğŸŒŸ 5. ë°ì´í„° Pineconeì— ì—…ì„œíŠ¸
def upload_to_pinecone(index, texts, metas, embedding_model, batch_size=100):
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i + batch_size]
        batch_metas = metas[i:i + batch_size]
        
        ids = [str(uuid4()) for _ in batch_texts]
        embeddings = embedding_model.embed_documents(batch_texts)
        
        index.upsert(vectors=zip(ids, embeddings, batch_metas))
        time.sleep(1)  # ê³¼ë„í•œ ìš”ì²­ ë°©ì§€


# ğŸŒŸ 6. Pinecone ë²¡í„° ì €ì¥ì†Œ ì„¤ì •
def save_to_pinecone(index, embedding_model):
    vectorstore = PineconeVectorStore(
        index=index,
        embedding=embedding_model,
        text_key="chunk"
    )
    print(f"âœ… ë°ì´í„°ê°€ Pinecone ì¸ë±ìŠ¤ '{index_name}'ì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


# ğŸŒŸ 7. ë©”ì¸ í•¨ìˆ˜
def main():
    print("âœ… Pinecone ì¸ë±ìŠ¤ ì„¤ì • ì¤‘...")
    index = setup_pinecone(pinecone_api_key)
    
    print("âœ… ë°ì´í„° ë¡œë“œ ì¤‘...")
    docs = load_documents('df_dart.csv')
    
    print("âœ… ë¬¸ì„œ ë¶„í•  ì¤‘...")
    texts, metas = split_documents(docs)
    
    print("âœ… Pineconeì— ë°ì´í„° ì—…ë¡œë“œ ì¤‘...")
    upload_to_pinecone(index, texts, metas, embedding_upstage)
    
    print("âœ… Pinecone ë²¡í„° ì €ì¥ì†Œ ì„¤ì • ì¤‘...")
    save_to_pinecone(index, embedding_upstage)
    
    print("ğŸ¯ ëª¨ë“  ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")


# ğŸŒŸ 8. í”„ë¡œê·¸ë¨ ì‹¤í–‰
if __name__ == "__main__":
    main()
